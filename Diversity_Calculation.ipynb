{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9232a96e-b0f9-406b-9ef7-044f5ddcdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb6b90-dd1a-4984-83f5-27aec16bcbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf1406-a8f9-45d6-81be-31f2057271e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f61679d8-c4fc-44fd-8152-06ccfd71429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('synthetic_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33ca73e7-587b-452e-9d49-d3a3f7ebd077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prompt</th>\n",
       "      <th>reply</th>\n",
       "      <th>name</th>\n",
       "      <th>platform</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>nationality</th>\n",
       "      <th>income</th>\n",
       "      <th>occupation</th>\n",
       "      <th>State</th>\n",
       "      <th>education</th>\n",
       "      <th>relationship</th>\n",
       "      <th>hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Hello LinkedIn Community,\\n\\nMy name is Ramdia...</td>\n",
       "      <td>RAMDIAL HAMZI</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>70</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>70290</td>\n",
       "      <td>Court Reporters and Simultaneous Captioners</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Hello Reddit community,\\n\\nAllow me to introdu...</td>\n",
       "      <td>RAMDIAL HAMZI</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>70</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>70290</td>\n",
       "      <td>Court Reporters and Simultaneous Captioners</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Hello everyone,\\n\\nMy name is Ramdial Hamzi, a...</td>\n",
       "      <td>RAMDIAL HAMZI</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>70</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>70290</td>\n",
       "      <td>Court Reporters and Simultaneous Captioners</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Hello, world! My name is Ramdial Hamzi. I’m a ...</td>\n",
       "      <td>RAMDIAL HAMZI</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>70</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>70290</td>\n",
       "      <td>Court Reporters and Simultaneous Captioners</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Hello LinkedIn Community,\\n\\nToday, I'd like t...</td>\n",
       "      <td>MOHAMMADALI ABUKHALIL</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>60</td>\n",
       "      <td>Male</td>\n",
       "      <td>American Indian</td>\n",
       "      <td>United States</td>\n",
       "      <td>45750</td>\n",
       "      <td>Roustabouts, Oil and Gas</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>35</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Just came back from my latest journey across t...</td>\n",
       "      <td>MANTANA YUM</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>95</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>United States</td>\n",
       "      <td>53350</td>\n",
       "      <td>Sailors and Marine Oilers</td>\n",
       "      <td>Washington</td>\n",
       "      <td>High school diploma</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>36</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>**LinkedIn Post:**\\n\\nHello, LinkedIn communit...</td>\n",
       "      <td>MOULOUD RAMOO</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>United States</td>\n",
       "      <td>96650</td>\n",
       "      <td>Agricultural Sciences Teachers, Postsecondary</td>\n",
       "      <td>Maine</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>37</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Title: Discovering Peace in the Gardens\\n\\nHey...</td>\n",
       "      <td>MOULOUD RAMOO</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>United States</td>\n",
       "      <td>96650</td>\n",
       "      <td>Agricultural Sciences Teachers, Postsecondary</td>\n",
       "      <td>Maine</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>38</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>I've been reflecting a lot on the past year, e...</td>\n",
       "      <td>MOULOUD RAMOO</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>United States</td>\n",
       "      <td>96650</td>\n",
       "      <td>Agricultural Sciences Teachers, Postsecondary</td>\n",
       "      <td>Maine</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>39</td>\n",
       "      <td>In this task, pretend to be a fictitious perso...</td>\n",
       "      <td>Just wrapped up an enlightening lecture on sus...</td>\n",
       "      <td>MOULOUD RAMOO</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>United States</td>\n",
       "      <td>96650</td>\n",
       "      <td>Agricultural Sciences Teachers, Postsecondary</td>\n",
       "      <td>Maine</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                             prompt  \\\n",
       "0             0  In this task, pretend to be a fictitious perso...   \n",
       "1             1  In this task, pretend to be a fictitious perso...   \n",
       "2             2  In this task, pretend to be a fictitious perso...   \n",
       "3             3  In this task, pretend to be a fictitious perso...   \n",
       "4             0  In this task, pretend to be a fictitious perso...   \n",
       "..          ...                                                ...   \n",
       "215          35  In this task, pretend to be a fictitious perso...   \n",
       "216          36  In this task, pretend to be a fictitious perso...   \n",
       "217          37  In this task, pretend to be a fictitious perso...   \n",
       "218          38  In this task, pretend to be a fictitious perso...   \n",
       "219          39  In this task, pretend to be a fictitious perso...   \n",
       "\n",
       "                                                 reply                   name  \\\n",
       "0    Hello LinkedIn Community,\\n\\nMy name is Ramdia...          RAMDIAL HAMZI   \n",
       "1    Hello Reddit community,\\n\\nAllow me to introdu...          RAMDIAL HAMZI   \n",
       "2    Hello everyone,\\n\\nMy name is Ramdial Hamzi, a...          RAMDIAL HAMZI   \n",
       "3    Hello, world! My name is Ramdial Hamzi. I’m a ...          RAMDIAL HAMZI   \n",
       "4    Hello LinkedIn Community,\\n\\nToday, I'd like t...  MOHAMMADALI ABUKHALIL   \n",
       "..                                                 ...                    ...   \n",
       "215  Just came back from my latest journey across t...            MANTANA YUM   \n",
       "216  **LinkedIn Post:**\\n\\nHello, LinkedIn communit...          MOULOUD RAMOO   \n",
       "217  Title: Discovering Peace in the Gardens\\n\\nHey...          MOULOUD RAMOO   \n",
       "218  I've been reflecting a lot on the past year, e...          MOULOUD RAMOO   \n",
       "219  Just wrapped up an enlightening lecture on sus...          MOULOUD RAMOO   \n",
       "\n",
       "     platform  age     sex             race    nationality  income  \\\n",
       "0    LinkedIn   70    Male            Black         Kuwait   70290   \n",
       "1      Reddit   70    Male            Black         Kuwait   70290   \n",
       "2    Facebook   70    Male            Black         Kuwait   70290   \n",
       "3     Twitter   70    Male            Black         Kuwait   70290   \n",
       "4    LinkedIn   60    Male  American Indian  United States   45750   \n",
       "..        ...  ...     ...              ...            ...     ...   \n",
       "215   Twitter   95    Male            Asian  United States   53350   \n",
       "216  LinkedIn   26  Female            Black  United States   96650   \n",
       "217    Reddit   26  Female            Black  United States   96650   \n",
       "218  Facebook   26  Female            Black  United States   96650   \n",
       "219   Twitter   26  Female            Black  United States   96650   \n",
       "\n",
       "                                        occupation       State  \\\n",
       "0      Court Reporters and Simultaneous Captioners   Wisconsin   \n",
       "1      Court Reporters and Simultaneous Captioners   Wisconsin   \n",
       "2      Court Reporters and Simultaneous Captioners   Wisconsin   \n",
       "3      Court Reporters and Simultaneous Captioners   Wisconsin   \n",
       "4                         Roustabouts, Oil and Gas   Louisiana   \n",
       "..                                             ...         ...   \n",
       "215                      Sailors and Marine Oilers  Washington   \n",
       "216  Agricultural Sciences Teachers, Postsecondary       Maine   \n",
       "217  Agricultural Sciences Teachers, Postsecondary       Maine   \n",
       "218  Agricultural Sciences Teachers, Postsecondary       Maine   \n",
       "219  Agricultural Sciences Teachers, Postsecondary       Maine   \n",
       "\n",
       "               education   relationship  hardness  \n",
       "0       Doctorate degree  Never Married         1  \n",
       "1       Doctorate degree  Never Married         1  \n",
       "2       Doctorate degree  Never Married         1  \n",
       "3       Doctorate degree  Never Married         1  \n",
       "4       Associate degree        Widowed         2  \n",
       "..                   ...            ...       ...  \n",
       "215  High school diploma       Divorced         5  \n",
       "216     Associate degree  Never Married         5  \n",
       "217     Associate degree  Never Married         5  \n",
       "218     Associate degree  Never Married         5  \n",
       "219     Associate degree  Never Married         5  \n",
       "\n",
       "[220 rows x 15 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb272fc-6910-4797-8dcb-789bb49541b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# import sacrebleu\n",
    "\n",
    "# # Define reference and candidate sentences\n",
    "# reference = [['this is a test']]  # List of strings (each reference sentence)\n",
    "# candidate = 'this is test'        # A single string (the candidate sentence)\n",
    "\n",
    "# # Calculate BLEU score using sacrebleu\n",
    "# bleu_score = sacrebleu.corpus_bleu([candidate], reference)\n",
    "\n",
    "# print(f\"BLEU score: {bleu_score.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3ef776-ac63-44ba-a2ff-a868c553a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score indicating text diversity: 1.4913\n"
     ]
    }
   ],
   "source": [
    "# Function to compute BLEU score between two texts\n",
    "def compute_bleu(reference, candidate):\n",
    "    # SacreBLEU expects a list of references and a single candidate string\n",
    "    bleu_score = sacrebleu.corpus_bleu([candidate], [[reference]])\n",
    "    return bleu_score.score\n",
    "\n",
    "# Calculate BLEU scores for each pair of texts\n",
    "bleu_scores = []\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        reference = df['reply'].iloc[i]\n",
    "        candidate = df['reply'].iloc[j]\n",
    "        score = compute_bleu(reference, candidate)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "# Calculate the average BLEU score\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "print(f\"Average BLEU score indicating text diversity: {average_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e080f5-cc5e-45d4-9177-93daeb60afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score Range and Interpretation\n",
    "# 0: No overlap between the candidate text and reference text(s).\n",
    "# 1-10: Very low overlap; the candidate is significantly different from the reference(s).\n",
    "# 10-30: Low overlap; the candidate is somewhat different but may still contain some similar words or phrases.\n",
    "# 30-50: Moderate overlap; the candidate captures a fair amount of the content or style of the reference but with noticeable differences.\n",
    "# 50-70: High overlap; the candidate is quite similar to the reference in terms of phrasing and word choice.\n",
    "# 70-100: Very high overlap; the candidate is almost identical to the reference(s). A perfect match would yield a BLEU score of 100, but this is rare in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d238c9-8f90-42f1-9c0a-23d1317825fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6bd56be-2295-4be3-b5e6-76295cef21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e95118c-f1c5-484f-be19-3a974eca3f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pnvenkit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d405454b-d3d5-4239-976c-27654c2352f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lexical Diversity: Type-Token Ratio (TTR) ###\n",
    "def calculate_ttr(text_column):\n",
    "    tokens = [word for text in text_column for word in nltk.word_tokenize(text)]\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    num_total_tokens = len(tokens)\n",
    "    ttr = num_unique_tokens / num_total_tokens if num_total_tokens > 0 else 0\n",
    "    return ttr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5548f8e0-2257-4a64-9ddd-d2ad5fe1f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Syntactic Diversity: Average Sentence Length ###\n",
    "def calculate_avg_sentence_length(text_column):\n",
    "    sentence_lengths = [len(nltk.word_tokenize(sentence)) for text in text_column for sentence in nltk.sent_tokenize(text)]\n",
    "    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    return avg_sentence_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94abf419-6c4b-4281-8c70-55e5cabc3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Semantic Diversity: Cosine Similarity ###\n",
    "def calculate_avg_cosine_similarity(text_column):\n",
    "    # Initialize the model to compute embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Compute sentence embeddings for each text\n",
    "    embeddings = model.encode(text_column)\n",
    "    \n",
    "    # Compute pairwise cosine similarity\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Compute average cosine similarity, ignoring self-similarities\n",
    "    avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "    return avg_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f5667c-b57e-4f85-af32-e8ccdc5880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Self-BLEU Score for Diversity ###\n",
    "def calculate_self_bleu(text_column):\n",
    "    bleu_scores = []\n",
    "    smooth_func = SmoothingFunction().method4  # Using a smoothing method\n",
    "\n",
    "    # Calculate BLEU score for each text as a candidate\n",
    "    for i, candidate in enumerate(text_column):\n",
    "        references = [text.split() for j, text in enumerate(text_column) if j != i]  # Exclude the candidate itself\n",
    "        candidate_tokens = candidate.split()\n",
    "        \n",
    "        # Calculate BLEU score using other texts as references\n",
    "        score = sentence_bleu(references, candidate_tokens, smoothing_function=smooth_func)\n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    # Return the average Self-BLEU score\n",
    "    average_self_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    return average_self_bleu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9556c604-69ce-4e6a-a1bc-61b4845b7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute diversity metrics\n",
    "ttr = calculate_ttr(df['reply'])\n",
    "avg_sentence_length = calculate_avg_sentence_length(df['reply'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59e2b064-1a7b-4b46-a1e1-a01717484d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cosine_similarity = calculate_avg_cosine_similarity(df['reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16437126-ce45-4ce1-9353-f70f3b5cdf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity (Type-Token Ratio): 0.528\n",
      "Syntactic Diversity (Average Sentence Length): 10.60\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Lexical Diversity (Type-Token Ratio): {ttr:.3f}\")\n",
    "print(f\"Syntactic Diversity (Average Sentence Length): {avg_sentence_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3530c0bb-0bed-4611-86b5-241bb5ff2243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Diversity (Average Cosine Similarity): 0.164\n"
     ]
    }
   ],
   "source": [
    "print(f\"Semantic Diversity (Average Cosine Similarity): {avg_cosine_similarity:.3f}\")\n",
    "# print(f\"Average Self-BLEU Score: {self_bleu_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa70f99f-1254-4c48-8b53-e52a533cf8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity (Type-Token Ratio): 0.528\n",
      "Syntactic Diversity (Average Sentence Length): 10.60\n",
      "Semantic Diversity (Average Cosine Similarity): 0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pnvenkit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example DataFrame with a column containing text data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"A fast brown fox leaps over a sleepy dog.\",\n",
    "        \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"In a faraway land, a fox swiftly jumps over a dog.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "### Lexical Diversity: Type-Token Ratio (TTR) ###\n",
    "def calculate_ttr(text_column):\n",
    "    tokens = [word for text in text_column for word in nltk.word_tokenize(text)]\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    num_total_tokens = len(tokens)\n",
    "    ttr = num_unique_tokens / num_total_tokens if num_total_tokens > 0 else 0\n",
    "    return ttr\n",
    "\n",
    "### Syntactic Diversity: Average Sentence Length ###\n",
    "def calculate_avg_sentence_length(text_column):\n",
    "    sentence_lengths = [len(nltk.word_tokenize(sentence)) for text in text_column for sentence in nltk.sent_tokenize(text)]\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
    "    return avg_sentence_length\n",
    "\n",
    "### Semantic Diversity: Cosine Similarity Using TF-IDF ###\n",
    "def calculate_avg_cosine_similarity(text_column):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_column)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Compute average cosine similarity, ignoring self-similarities\n",
    "    num_elements = len(text_column)\n",
    "    sum_similarity = sum(similarity_matrix[i][j] for i in range(num_elements) for j in range(i + 1, num_elements))\n",
    "    num_pairs = (num_elements * (num_elements - 1)) / 2\n",
    "\n",
    "    avg_similarity = sum_similarity / num_pairs if num_pairs else 0\n",
    "    return avg_similarity\n",
    "\n",
    "\n",
    "# Compute diversity metrics\n",
    "ttr = calculate_ttr(df['text'])\n",
    "avg_sentence_length = calculate_avg_sentence_length(df['text'])\n",
    "avg_cosine_similarity = calculate_avg_cosine_similarity(df['text'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Lexical Diversity (Type-Token Ratio): {ttr:.3f}\")\n",
    "print(f\"Syntactic Diversity (Average Sentence Length): {avg_sentence_length:.2f}\")\n",
    "print(f\"Semantic Diversity (Average Cosine Similarity): {avg_cosine_similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b301144-efc2-4149-9681-4d56fb7fa812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
